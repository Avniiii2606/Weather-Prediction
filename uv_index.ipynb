{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avniiii2606/Weather-Prediction/blob/main/uv_index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "nelgiriyewithana_global_weather_repository_path = kagglehub.dataset_download('nelgiriyewithana/global-weather-repository')\n",
        "\n",
        "print('Data source import complete.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt4nH446hPLn",
        "outputId": "84d2981e-3589-4348-ef99-6f3db7ac1049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.3)\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/nelgiriyewithana/global-weather-repository?dataset_version_number=382...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.26M/2.26M [00:00<00:00, 148MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Data source import complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-19T07:40:56.462463Z",
          "iopub.execute_input": "2024-10-19T07:40:56.463687Z",
          "iopub.status.idle": "2024-10-19T07:40:56.953762Z",
          "shell.execute_reply.started": "2024-10-19T07:40:56.463628Z",
          "shell.execute_reply": "2024-10-19T07:40:56.952419Z"
        },
        "id": "Oktdss35xawI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"nelgiriyewithana/global-weather-repository\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-19T07:43:32.585644Z",
          "iopub.execute_input": "2024-10-19T07:43:32.586677Z",
          "iopub.status.idle": "2024-10-19T07:43:32.889003Z",
          "shell.execute_reply.started": "2024-10-19T07:43:32.586628Z",
          "shell.execute_reply": "2024-10-19T07:43:32.887931Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gPdPFluxawK",
        "outputId": "a655409a-47c0-418a-88aa-5c299c8dc026"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.3)\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/nelgiriyewithana/global-weather-repository/versions/382\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score,confusion_matrix\n"
      ],
      "metadata": {
        "id": "DCXfgM1QJiBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"nelgiriyewithana/global-weather-repository\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "\n",
        "# Path to the dataset version directory\n",
        "dataset_version_dir = '/root/.cache/kagglehub/datasets/nelgiriyewithana/global-weather-repository/versions/381'\n",
        "\n",
        "# Find the CSV file within the directory\n",
        "for filename in os.listdir(dataset_version_dir):\n",
        "    if filename.endswith('.csv'):\n",
        "        csv_file_path = os.path.join(dataset_version_dir, filename)\n",
        "        break  # Stop after finding the first CSV file\n",
        "\n",
        "# Now load the CSV file using the correct path\n",
        "data = pd.read_csv(csv_file_path)\n",
        "\n",
        "import folium\n",
        "import pandas as pd\n",
        "from folium.plugins import HeatMap\n",
        "from IPython.display import display\n",
        "\n",
        "# The CSV file is within the downloaded dataset version directory\n",
        "dataset_version_dir = '/root/.cache/kagglehub/datasets/nelgiriyewithana/global-weather-repository/versions/381'\n",
        "\n",
        "# Find the CSV file within the directory\n",
        "for filename in os.listdir(dataset_version_dir):\n",
        "    if filename.endswith('.csv'):\n",
        "        csv_file_path = os.path.join(dataset_version_dir, filename)\n",
        "        break  # Stop after finding the first CSV file\n",
        "df = pd.read_csv(csv_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "hy4x_gFZyuDf",
        "outputId": "a43ae08d-2b37-45da-95a4-061e823857bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.3)\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/nelgiriyewithana/global-weather-repository/versions/382\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/root/.cache/kagglehub/datasets/nelgiriyewithana/global-weather-repository/versions/381'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-01471121ae5c>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Find the CSV file within the directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_version_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mcsv_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_version_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/.cache/kagglehub/datasets/nelgiriyewithana/global-weather-repository/versions/381'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_search = df['timezone']\n",
        "value=\"Asia/\"\n",
        "matches = list(filter(lambda s: value in s, df_search))\n",
        "unique_values_column1 = pd.unique(matches)\n",
        "print(f\"Strings containing',{unique_values_column1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "q_20PRDVWp5B",
        "outputId": "2d09f39c-1bf1-4ea0-dfb2-71d5e1166ea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5b59fc816d60>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timezone'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Asia/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_search\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0munique_values_column1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Strings containing',{unique_values_column1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_search = df['timezone']\n",
        "unique_values_column1 = pd.unique(df_search)\n",
        "print(f\"Unique values': {unique_values_column1}\")"
      ],
      "metadata": {
        "id": "qDQYPvLHQ8Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Total=['Asia/Kabul' ,'Asia/Yerevan' ,'Asia/Baku' ,'Asia/Bahrain', 'Asia/Dhaka',\n",
        " 'Asia/Thimphu', 'Asia/Brunei' ,'Asia/Phnom_Penh', 'Asia/Shanghai',\n",
        " 'Asia/Famagusta', 'Asia/Tbilisi' ,'Asia/Kolkata', 'Asia/Jakarta',\n",
        " 'Asia/Tehran', 'Asia/Baghdad' ,'Asia/Jerusalem', 'Asia/Amman',\n",
        " 'Asia/Almaty', 'Asia/Kuwait', 'Asia/Bishkek',  'Asia/Bangkok',\n",
        " 'Asia/Kuala_Lumpur', 'Asia/Ulaanbaatar' ,'Asia/Yangon' ,'Asia/Kathmandu',\n",
        " 'Asia/Pyongyang', 'Asia/Muscat', 'Asia/Karachi', 'Asia/Manila', 'Asia/Qatar',\n",
        " 'Asia/Riyadh' ,'Asia/Singapore', 'Asia/Seoul', 'Asia/Colombo',\n",
        " 'Asia/Damascus', 'Asia/Dushanbe' ,'Asia/Dili' ,'Asia/Ashgabat' ,'Asia/Dubai',\n",
        " 'Asia/Tashkent' ,'Asia/Aden'  , 'Asia/Tokyo','Asia/Beirut']\n",
        "\n",
        "# removing 'Asia/Ho_Chi_Minh' and 'Asia/Kuching' as they have very few values\n",
        "mse_results = []\n",
        "r2_results = []\n",
        "\n",
        "for i in range(len(Total)):\n",
        "  dfi= df.loc[df['timezone'] == Total[i]]\n",
        "  dfi.reset_index(drop=True, inplace=True)\n",
        "  dfi = dfi.drop(columns=['country', 'location_name', 'latitude', 'longitude', 'timezone', 'last_updated_epoch','last_updated',\n",
        "                      'condition_text','feels_like_celsius','feels_like_fahrenheit','visibility_km','visibility_miles','uv_index',\n",
        "                     'sunrise','sunset','moonrise','moonset','moon_phase','moon_illumination' ,'wind_direction'])\n",
        "  dfi.ffill( inplace=True)\n",
        "  X=dfi.drop(columns=['air_quality_PM2.5'])\n",
        "  y=dfi['air_quality_PM2.5']\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  model =LinearRegression()\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  mse = mean_squared_error(y_test, y_pred)\n",
        "  r2 = r2_score(y_test, y_pred)\n",
        "  mse_results.append(mse)\n",
        "  r2_results.append(r2)\n",
        "  print(f'Timezone: {Total[i]}')\n",
        "  print(f'Mean Squared Error: {mse}')\n",
        "  print(f'R-squared: {r2}')\n",
        "  print(\"\")\n",
        "MSE = pd.DataFrame(mse_results)\n",
        "r2 = pd.DataFrame(r2_results)\n",
        "sumMSE = MSE.sum()\n",
        "lenMSE= len(MSE)\n",
        "sumR2 = r2.sum()\n",
        "lenR2 = len(r2)\n",
        "print(f'Average Mean Squared Error: ',sumMSE/lenMSE)\n",
        "print(f'Average R-squared: ',sumR2/lenR2)\n"
      ],
      "metadata": {
        "id": "7Ar4r4lOXEOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickel\n",
        "\n",
        "with open('model_uv.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ],
      "metadata": {
        "id": "YiA8-ck4TWJJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "71c4c1ca-d826-47d8-e10c-0fb6d9965272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pickel'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c0e23b473b68>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpickel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_uv.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pickel'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}